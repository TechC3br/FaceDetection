{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a367a705-ebcb-44cd-8e10-e07e23c9a7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\souja\\anaconda3\\envs\\project\\lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "\n",
    "pyautogui.FAILSAFE = False\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# Get screen dimensions\n",
    "screen_width, screen_height = pyautogui.size()\n",
    "\n",
    "# Capture video from the webcam\n",
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "# Initialize last known cursor position\n",
    "last_screen_x, last_screen_y = screen_width // 2, screen_height // 2  # Start in the center of the screen\n",
    "\n",
    "with mp_hands.Hands(model_complexity=0, min_detection_confidence=0.5, min_tracking_confidence=0.5) as hands:\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            continue\n",
    "        \n",
    "        # Flip the image horizontally and convert to RGB\n",
    "        image = cv2.flip(image, 1)\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Process the image and detect hands\n",
    "        results = hands.process(image_rgb)\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                # Get hand landmarks\n",
    "                handLandmarks = []\n",
    "                for landmarks in hand_landmarks.landmark:\n",
    "                    handLandmarks.append([landmarks.x, landmarks.y])\n",
    "\n",
    "                # Control mouse movement with the tip of the index finger (landmark 8)\n",
    "                x = int(handLandmarks[8][0] * image.shape[1])  # X-coordinate in the frame\n",
    "                y = int(handLandmarks[8][1] * image.shape[0])  # Y-coordinate in the frame\n",
    "\n",
    "                # Scale the coordinates to the screen size\n",
    "                screen_x = int(x * screen_width / image.shape[1])\n",
    "                screen_y = int(y * screen_height / image.shape[0])\n",
    "\n",
    "                # Move the mouse cursor only if hand is detected\n",
    "                last_screen_x, last_screen_y = screen_x, screen_y\n",
    "                pyautogui.moveTo(last_screen_x, last_screen_y)\n",
    "\n",
    "                # Detect click gestures\n",
    "                if handLandmarks[4][1] < handLandmarks[3][1] and abs(handLandmarks[4][0] - handLandmarks[8][0]) < 0.05:\n",
    "                    pyautogui.click()\n",
    "                elif handLandmarks[4][1] < handLandmarks[3][1] and abs(handLandmarks[4][0] - handLandmarks[12][0]) < 0.05:\n",
    "                    pyautogui.click(button='right')\n",
    "\n",
    "                # Draw hand landmarks\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image, hand_landmarks, mp_hands.HAND_CONNECTIONS,\n",
    "                    mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                    mp_drawing_styles.get_default_hand_connections_style()\n",
    "                )\n",
    "        else:\n",
    "            # If no hand detected, maintain last known cursor position\n",
    "            pyautogui.moveTo(last_screen_x, last_screen_y)\n",
    "\n",
    "        # Display the video feed with landmarks\n",
    "        cv2.imshow('Hand Gesture Mouse Control', image)\n",
    "\n",
    "        # Exit if 'q' is pressed\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98920bde-7446-4222-a76b-d173f4366944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "\n",
    "pyautogui.FAILSAFE = False\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# Get screen dimensions\n",
    "screen_width, screen_height = pyautogui.size()\n",
    "\n",
    "# Capture video from the webcam\n",
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "# Initialize last known cursor position\n",
    "last_screen_x, last_screen_y = screen_width // 2, screen_height // 2  # Start in the center of the screen\n",
    "\n",
    "with mp_hands.Hands(model_complexity=0, min_detection_confidence=0.5, min_tracking_confidence=0.5) as hands:\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            continue\n",
    "        \n",
    "        # Flip the image horizontally and convert to RGB\n",
    "        image = cv2.flip(image, 1)\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Process the image and detect hands\n",
    "        results = hands.process(image_rgb)\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                # Get hand landmarks\n",
    "                handLandmarks = []\n",
    "                for landmarks in hand_landmarks.landmark:\n",
    "                    handLandmarks.append([landmarks.x, landmarks.y])\n",
    "\n",
    "                # Control mouse movement with the tip of the index finger (landmark 8)\n",
    "                x = int(handLandmarks[8][0] * image.shape[1])  # X-coordinate in the frame\n",
    "                y = int(handLandmarks[8][1] * image.shape[0])  # Y-coordinate in the frame\n",
    "\n",
    "                # Scale the coordinates to the screen size\n",
    "                screen_x = int(x * screen_width / image.shape[1])\n",
    "                screen_y = int(y * screen_height / image.shape[0])\n",
    "\n",
    "                # Move the mouse cursor only if hand is detected\n",
    "                last_screen_x, last_screen_y = screen_x, screen_y\n",
    "                pyautogui.moveTo(last_screen_x, last_screen_y)\n",
    "\n",
    "                # Detect click gestures\n",
    "                if handLandmarks[4][1] < handLandmarks[3][1] and abs(handLandmarks[4][0] - handLandmarks[8][0]) < 0.05:\n",
    "                    pyautogui.click()\n",
    "                elif handLandmarks[4][1] < handLandmarks[3][1] and abs(handLandmarks[4][0] - handLandmarks[12][0]) < 0.05:\n",
    "                    pyautogui.click(button='right')\n",
    "\n",
    "                # Draw hand landmarks\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image, hand_landmarks, mp_hands.HAND_CONNECTIONS,\n",
    "                    mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                    mp_drawing_styles.get_default_hand_connections_style()\n",
    "                )\n",
    "        \n",
    "        # If no hand detected, do not move the cursor\n",
    "        # Optionally, you can comment out the last cursor position update\n",
    "        # last_screen_x, last_screen_y remains unchanged\n",
    "\n",
    "        # Display the video feed with landmarks\n",
    "        cv2.imshow('Hand Gesture Mouse Control', image)\n",
    "\n",
    "        # Exit if 'q' is pressed\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d00717a-c2f1-4d39-bd11-16c7838b04db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latency: 0.317864 seconds\n",
      "Latency: 0.031560 seconds\n",
      "Latency: 0.032115 seconds\n",
      "Latency: 0.031228 seconds\n",
      "Latency: 0.031821 seconds\n",
      "Latency: 0.032746 seconds\n",
      "Latency: 0.031480 seconds\n",
      "Latency: 0.017567 seconds\n",
      "Latency: 0.016058 seconds\n",
      "Latency: 0.026412 seconds\n",
      "Latency: 0.032237 seconds\n",
      "Latency: 0.033961 seconds\n",
      "Latency: 0.031828 seconds\n",
      "Latency: 0.034490 seconds\n",
      "Latency: 0.034473 seconds\n",
      "Latency: 0.031245 seconds\n",
      "Latency: 0.031467 seconds\n",
      "Latency: 0.036742 seconds\n",
      "Latency: 0.021006 seconds\n",
      "Latency: 0.031227 seconds\n",
      "Latency: 0.031336 seconds\n",
      "Latency: 0.031203 seconds\n",
      "Latency: 0.022368 seconds\n",
      "Latency: 0.031811 seconds\n",
      "Latency: 0.031129 seconds\n",
      "Latency: 0.031262 seconds\n",
      "Latency: 0.036086 seconds\n",
      "Latency: 0.038746 seconds\n",
      "Latency: 0.031188 seconds\n",
      "Latency: 0.031328 seconds\n",
      "Latency: 0.027472 seconds\n",
      "Latency: 0.038706 seconds\n",
      "Latency: 0.031177 seconds\n",
      "Latency: 0.031256 seconds\n",
      "Latency: 0.034199 seconds\n",
      "Latency: 0.032442 seconds\n",
      "Latency: 0.031125 seconds\n",
      "Latency: 0.033834 seconds\n",
      "Latency: 0.038625 seconds\n",
      "Latency: 0.020900 seconds\n",
      "Latency: 0.031265 seconds\n",
      "Latency: 0.031507 seconds\n",
      "Latency: 0.031200 seconds\n",
      "Latency: 0.031846 seconds\n",
      "Latency: 0.019082 seconds\n",
      "Latency: 0.029837 seconds\n",
      "Latency: 0.031280 seconds\n",
      "Latency: 0.031554 seconds\n",
      "Latency: 0.031122 seconds\n",
      "Latency: 0.022451 seconds\n",
      "Latency: 0.043773 seconds\n",
      "Latency: 0.033793 seconds\n",
      "Latency: 0.031336 seconds\n",
      "Latency: 0.031180 seconds\n",
      "Latency: 0.024308 seconds\n",
      "Latency: 0.019754 seconds\n",
      "Latency: 0.031755 seconds\n",
      "Latency: 0.031128 seconds\n",
      "Latency: 0.031734 seconds\n",
      "Latency: 0.022648 seconds\n",
      "Latency: 0.037081 seconds\n",
      "Latency: 0.031199 seconds\n",
      "Latency: 0.031705 seconds\n",
      "Latency: 0.031251 seconds\n",
      "Latency: 0.020251 seconds\n",
      "Latency: 0.002265 seconds\n",
      "Latency: 0.015634 seconds\n",
      "Latency: 0.118723 seconds\n",
      "Latency: 0.267917 seconds\n",
      "Latency: 0.250068 seconds\n",
      "Latency: 0.487280 seconds\n",
      "Latency: 0.231003 seconds\n",
      "Latency: 0.251159 seconds\n",
      "Latency: 0.249428 seconds\n",
      "Latency: 0.234365 seconds\n",
      "Latency: 0.235845 seconds\n",
      "Latency: 0.234721 seconds\n",
      "Latency: 0.236204 seconds\n",
      "Latency: 0.232434 seconds\n",
      "Latency: 0.235398 seconds\n",
      "Latency: 0.235145 seconds\n",
      "Latency: 0.235185 seconds\n",
      "Latency: 0.234407 seconds\n",
      "Latency: 0.235794 seconds\n",
      "Latency: 0.235011 seconds\n",
      "Latency: 0.251297 seconds\n",
      "Latency: 0.230933 seconds\n",
      "Latency: 0.235009 seconds\n",
      "Latency: 0.235085 seconds\n",
      "Latency: 0.455935 seconds\n",
      "Latency: 0.212840 seconds\n",
      "Latency: 0.235708 seconds\n",
      "Latency: 0.234397 seconds\n",
      "Latency: 0.236317 seconds\n",
      "Latency: 0.250320 seconds\n",
      "Latency: 0.250613 seconds\n",
      "Latency: 0.248039 seconds\n",
      "Latency: 0.127942 seconds\n",
      "Latency: 0.235489 seconds\n",
      "Latency: 0.254156 seconds\n",
      "Latency: 0.225633 seconds\n",
      "Latency: 0.246505 seconds\n",
      "Latency: 0.235231 seconds\n",
      "Latency: 0.234642 seconds\n",
      "Latency: 0.019705 seconds\n",
      "Latency: 0.017942 seconds\n",
      "Latency: 0.018546 seconds\n",
      "Latency: 0.033150 seconds\n",
      "Latency: 0.018058 seconds\n",
      "Latency: 0.027234 seconds\n",
      "Latency: 0.021191 seconds\n",
      "Latency: 0.017360 seconds\n",
      "Latency: 0.018226 seconds\n",
      "Latency: 0.017319 seconds\n",
      "Latency: 0.017704 seconds\n",
      "Latency: 0.020399 seconds\n",
      "Latency: 0.018108 seconds\n",
      "Latency: 0.018970 seconds\n",
      "Latency: 0.001599 seconds\n",
      "Latency: 0.016407 seconds\n",
      "Latency: 0.007204 seconds\n",
      "Latency: 0.014223 seconds\n",
      "Latency: 0.017607 seconds\n",
      "Latency: 0.007208 seconds\n",
      "Latency: 0.001930 seconds\n",
      "Latency: 0.017685 seconds\n",
      "Latency: 0.019995 seconds\n",
      "Latency: 0.017184 seconds\n",
      "Latency: 0.017256 seconds\n",
      "Latency: 0.018866 seconds\n",
      "Latency: 0.017257 seconds\n",
      "Latency: 0.004295 seconds\n",
      "Latency: 0.023266 seconds\n",
      "Latency: 0.025283 seconds\n",
      "Latency: 0.017755 seconds\n",
      "Latency: 0.018539 seconds\n",
      "Latency: 0.015812 seconds\n",
      "Latency: 0.021810 seconds\n",
      "Latency: 0.021531 seconds\n",
      "Latency: 0.018198 seconds\n",
      "Latency: 0.017540 seconds\n",
      "Latency: 0.018262 seconds\n",
      "Latency: 0.002054 seconds\n",
      "Latency: 0.020724 seconds\n",
      "Latency: 0.037840 seconds\n",
      "Latency: 0.017134 seconds\n",
      "Latency: 0.004556 seconds\n",
      "Latency: 0.002685 seconds\n",
      "Latency: 0.017496 seconds\n",
      "Latency: 0.256669 seconds\n",
      "Latency: 0.450124 seconds\n",
      "Latency: 0.110307 seconds\n",
      "Latency: 0.231824 seconds\n",
      "Latency: 0.128539 seconds\n",
      "Latency: 0.243886 seconds\n",
      "Latency: 0.237246 seconds\n",
      "Latency: 0.236065 seconds\n",
      "Latency: 0.233934 seconds\n",
      "Latency: 0.130108 seconds\n",
      "Collected Latencies:\n",
      "Run 1: 0.317864 seconds\n",
      "Run 2: 0.031560 seconds\n",
      "Run 3: 0.032115 seconds\n",
      "Run 4: 0.031228 seconds\n",
      "Run 5: 0.031821 seconds\n",
      "Run 6: 0.032746 seconds\n",
      "Run 7: 0.031480 seconds\n",
      "Run 8: 0.017567 seconds\n",
      "Run 9: 0.016058 seconds\n",
      "Run 10: 0.026412 seconds\n",
      "Run 11: 0.032237 seconds\n",
      "Run 12: 0.033961 seconds\n",
      "Run 13: 0.031828 seconds\n",
      "Run 14: 0.034490 seconds\n",
      "Run 15: 0.034473 seconds\n",
      "Run 16: 0.031245 seconds\n",
      "Run 17: 0.031467 seconds\n",
      "Run 18: 0.036742 seconds\n",
      "Run 19: 0.021006 seconds\n",
      "Run 20: 0.031227 seconds\n",
      "Run 21: 0.031336 seconds\n",
      "Run 22: 0.031203 seconds\n",
      "Run 23: 0.022368 seconds\n",
      "Run 24: 0.031811 seconds\n",
      "Run 25: 0.031129 seconds\n",
      "Run 26: 0.031262 seconds\n",
      "Run 27: 0.036086 seconds\n",
      "Run 28: 0.038746 seconds\n",
      "Run 29: 0.031188 seconds\n",
      "Run 30: 0.031328 seconds\n",
      "Run 31: 0.027472 seconds\n",
      "Run 32: 0.038706 seconds\n",
      "Run 33: 0.031177 seconds\n",
      "Run 34: 0.031256 seconds\n",
      "Run 35: 0.034199 seconds\n",
      "Run 36: 0.032442 seconds\n",
      "Run 37: 0.031125 seconds\n",
      "Run 38: 0.033834 seconds\n",
      "Run 39: 0.038625 seconds\n",
      "Run 40: 0.020900 seconds\n",
      "Run 41: 0.031265 seconds\n",
      "Run 42: 0.031507 seconds\n",
      "Run 43: 0.031200 seconds\n",
      "Run 44: 0.031846 seconds\n",
      "Run 45: 0.019082 seconds\n",
      "Run 46: 0.029837 seconds\n",
      "Run 47: 0.031280 seconds\n",
      "Run 48: 0.031554 seconds\n",
      "Run 49: 0.031122 seconds\n",
      "Run 50: 0.022451 seconds\n",
      "Run 51: 0.043773 seconds\n",
      "Run 52: 0.033793 seconds\n",
      "Run 53: 0.031336 seconds\n",
      "Run 54: 0.031180 seconds\n",
      "Run 55: 0.024308 seconds\n",
      "Run 56: 0.019754 seconds\n",
      "Run 57: 0.031755 seconds\n",
      "Run 58: 0.031128 seconds\n",
      "Run 59: 0.031734 seconds\n",
      "Run 60: 0.022648 seconds\n",
      "Run 61: 0.037081 seconds\n",
      "Run 62: 0.031199 seconds\n",
      "Run 63: 0.031705 seconds\n",
      "Run 64: 0.031251 seconds\n",
      "Run 65: 0.020251 seconds\n",
      "Run 66: 0.002265 seconds\n",
      "Run 67: 0.015634 seconds\n",
      "Run 68: 0.118723 seconds\n",
      "Run 69: 0.267917 seconds\n",
      "Run 70: 0.250068 seconds\n",
      "Run 71: 0.487280 seconds\n",
      "Run 72: 0.231003 seconds\n",
      "Run 73: 0.251159 seconds\n",
      "Run 74: 0.249428 seconds\n",
      "Run 75: 0.234365 seconds\n",
      "Run 76: 0.235845 seconds\n",
      "Run 77: 0.234721 seconds\n",
      "Run 78: 0.236204 seconds\n",
      "Run 79: 0.232434 seconds\n",
      "Run 80: 0.235398 seconds\n",
      "Run 81: 0.235145 seconds\n",
      "Run 82: 0.235185 seconds\n",
      "Run 83: 0.234407 seconds\n",
      "Run 84: 0.235794 seconds\n",
      "Run 85: 0.235011 seconds\n",
      "Run 86: 0.251297 seconds\n",
      "Run 87: 0.230933 seconds\n",
      "Run 88: 0.235009 seconds\n",
      "Run 89: 0.235085 seconds\n",
      "Run 90: 0.455935 seconds\n",
      "Run 91: 0.212840 seconds\n",
      "Run 92: 0.235708 seconds\n",
      "Run 93: 0.234397 seconds\n",
      "Run 94: 0.236317 seconds\n",
      "Run 95: 0.250320 seconds\n",
      "Run 96: 0.250613 seconds\n",
      "Run 97: 0.248039 seconds\n",
      "Run 98: 0.127942 seconds\n",
      "Run 99: 0.235489 seconds\n",
      "Run 100: 0.254156 seconds\n",
      "Run 101: 0.225633 seconds\n",
      "Run 102: 0.246505 seconds\n",
      "Run 103: 0.235231 seconds\n",
      "Run 104: 0.234642 seconds\n",
      "Run 105: 0.019705 seconds\n",
      "Run 106: 0.017942 seconds\n",
      "Run 107: 0.018546 seconds\n",
      "Run 108: 0.033150 seconds\n",
      "Run 109: 0.018058 seconds\n",
      "Run 110: 0.027234 seconds\n",
      "Run 111: 0.021191 seconds\n",
      "Run 112: 0.017360 seconds\n",
      "Run 113: 0.018226 seconds\n",
      "Run 114: 0.017319 seconds\n",
      "Run 115: 0.017704 seconds\n",
      "Run 116: 0.020399 seconds\n",
      "Run 117: 0.018108 seconds\n",
      "Run 118: 0.018970 seconds\n",
      "Run 119: 0.001599 seconds\n",
      "Run 120: 0.016407 seconds\n",
      "Run 121: 0.007204 seconds\n",
      "Run 122: 0.014223 seconds\n",
      "Run 123: 0.017607 seconds\n",
      "Run 124: 0.007208 seconds\n",
      "Run 125: 0.001930 seconds\n",
      "Run 126: 0.017685 seconds\n",
      "Run 127: 0.019995 seconds\n",
      "Run 128: 0.017184 seconds\n",
      "Run 129: 0.017256 seconds\n",
      "Run 130: 0.018866 seconds\n",
      "Run 131: 0.017257 seconds\n",
      "Run 132: 0.004295 seconds\n",
      "Run 133: 0.023266 seconds\n",
      "Run 134: 0.025283 seconds\n",
      "Run 135: 0.017755 seconds\n",
      "Run 136: 0.018539 seconds\n",
      "Run 137: 0.015812 seconds\n",
      "Run 138: 0.021810 seconds\n",
      "Run 139: 0.021531 seconds\n",
      "Run 140: 0.018198 seconds\n",
      "Run 141: 0.017540 seconds\n",
      "Run 142: 0.018262 seconds\n",
      "Run 143: 0.002054 seconds\n",
      "Run 144: 0.020724 seconds\n",
      "Run 145: 0.037840 seconds\n",
      "Run 146: 0.017134 seconds\n",
      "Run 147: 0.004556 seconds\n",
      "Run 148: 0.002685 seconds\n",
      "Run 149: 0.017496 seconds\n",
      "Run 150: 0.256669 seconds\n",
      "Run 151: 0.450124 seconds\n",
      "Run 152: 0.110307 seconds\n",
      "Run 153: 0.231824 seconds\n",
      "Run 154: 0.128539 seconds\n",
      "Run 155: 0.243886 seconds\n",
      "Run 156: 0.237246 seconds\n",
      "Run 157: 0.236065 seconds\n",
      "Run 158: 0.233934 seconds\n",
      "Run 159: 0.130108 seconds\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "import time\n",
    "\n",
    "pyautogui.FAILSAFE = False\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# Get screen dimensions\n",
    "screen_width, screen_height = pyautogui.size()\n",
    "\n",
    "# Capture video from the webcam\n",
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "# Initialize last known cursor position\n",
    "last_screen_x, last_screen_y = screen_width // 2, screen_height // 2  # Start in the center of the screen\n",
    "\n",
    "# Store latencies\n",
    "latencies = []\n",
    "\n",
    "with mp_hands.Hands(model_complexity=0, min_detection_confidence=0.5, min_tracking_confidence=0.5) as hands:\n",
    "    while cap.isOpened():\n",
    "        start_time = time.time()  # Start timing\n",
    "        \n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            continue\n",
    "        \n",
    "        # Flip the image horizontally and convert to RGB\n",
    "        image = cv2.flip(image, 1)\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Process the image and detect hands\n",
    "        results = hands.process(image_rgb)\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                # Get hand landmarks\n",
    "                handLandmarks = [[landmark.x, landmark.y] for landmark in hand_landmarks.landmark]\n",
    "\n",
    "                # Control mouse movement with the tip of the index finger (landmark 8)\n",
    "                x = int(handLandmarks[8][0] * image.shape[1])  # X-coordinate in the frame\n",
    "                y = int(handLandmarks[8][1] * image.shape[0])  # Y-coordinate in the frame\n",
    "\n",
    "                # Scale the coordinates to the screen size\n",
    "                screen_x = int(x * screen_width / image.shape[1])\n",
    "                screen_y = int(y * screen_height / image.shape[0])\n",
    "\n",
    "                # Move the mouse cursor only if hand is detected\n",
    "                last_screen_x, last_screen_y = screen_x, screen_y\n",
    "                pyautogui.moveTo(last_screen_x, last_screen_y)\n",
    "\n",
    "                # Detect click gestures\n",
    "                if handLandmarks[4][1] < handLandmarks[3][1] and abs(handLandmarks[4][0] - handLandmarks[8][0]) < 0.05:\n",
    "                    pyautogui.click()\n",
    "                elif handLandmarks[4][1] < handLandmarks[3][1] and abs(handLandmarks[4][0] - handLandmarks[12][0]) < 0.05:\n",
    "                    pyautogui.click(button='right')\n",
    "\n",
    "                # Draw hand landmarks\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image, hand_landmarks, mp_hands.HAND_CONNECTIONS,\n",
    "                    mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                    mp_drawing_styles.get_default_hand_connections_style()\n",
    "                )\n",
    "        \n",
    "        # Display the video feed with landmarks\n",
    "        cv2.imshow('Hand Gesture Mouse Control', image)\n",
    "\n",
    "        # Calculate and log latency\n",
    "        latency = time.time() - start_time\n",
    "        latencies.append(latency)\n",
    "        print(f\"Latency: {latency:.6f} seconds\")\n",
    "\n",
    "        # Exit if 'q' is pressed\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Print collected latencies when done\n",
    "print(\"Collected Latencies:\")\n",
    "for idx, latency in enumerate(latencies):\n",
    "    print(f\"Run {idx + 1}: {latency:.6f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba41bb64-f3d3-4920-a631-35556ae716e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "from pynput.mouse import Button, Controller as MouseController\n",
    "from pynput.keyboard import Controller as KeyboardController, Key\n",
    "\n",
    "# Initialize mediapipe hands and drawing utils\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mouse = MouseController()\n",
    "keyboard = KeyboardController()\n",
    "\n",
    "# Setup hand tracking\n",
    "hands = mp_hands.Hands(\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.7,\n",
    "    min_tracking_confidence=0.7\n",
    ")\n",
    "\n",
    "# Function to detect swipe direction\n",
    "def detect_swipe_direction(x_diff):\n",
    "    if x_diff > 50:\n",
    "        return \"right\"\n",
    "    elif x_diff < -50:\n",
    "        return \"left\"\n",
    "    return None\n",
    "\n",
    "# Open webcam feed\n",
    "cap = cv2.VideoCapture(0)\n",
    "prev_x, prev_y = 0, 0\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        print(\"Ignoring empty frame.\")\n",
    "        continue\n",
    "\n",
    "    # Flip the image horizontally for a selfie-view display\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    h, w, _ = frame.shape\n",
    "\n",
    "    # Convert the frame to RGB\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the frame and get hand landmarks\n",
    "    result = hands.process(rgb_frame)\n",
    "\n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand_landmarks in result.multi_hand_landmarks:\n",
    "            # Get landmark coordinates\n",
    "            index_finger = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "            thumb = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP]\n",
    "            middle_finger = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_TIP]\n",
    "            \n",
    "            # Get the coordinates in pixels\n",
    "            index_x, index_y = int(index_finger.x * w), int(index_finger.y * h)\n",
    "            thumb_x, thumb_y = int(thumb.x * w), int(thumb.y * h)\n",
    "            middle_x, middle_y = int(middle_finger.x * w), int(middle_finger.y * h)\n",
    "\n",
    "            # Move the cursor based on index finger position\n",
    "            mouse.position = (index_x, index_y)\n",
    "\n",
    "            # Calculate the difference between previous and current x-coordinates for swipes\n",
    "            x_diff = index_x - prev_x\n",
    "            swipe_direction = detect_swipe_direction(x_diff)\n",
    "\n",
    "            if swipe_direction == \"right\":\n",
    "                #print(\"Swiped Right\")\n",
    "                keyboard.press(Key.right)\n",
    "                keyboard.release(Key.right)\n",
    "            elif swipe_direction == \"left\":\n",
    "                #print(\"Swiped Left\")\n",
    "                keyboard.press(Key.left)\n",
    "                keyboard.release(Key.left)\n",
    "\n",
    "            # Scroll up or down based on thumb and index finger proximity\n",
    "            distance_thumb = abs(index_y - thumb_y)\n",
    "            if distance_thumb < 40:\n",
    "                # Scroll up if thumb is close to index finger\n",
    "                mouse.scroll(0, 2)\n",
    "            elif distance_thumb > 90 and distance_thumb<100:\n",
    "                # Scroll down if thumb is far from index finger\n",
    "                mouse.scroll(0, -2)\n",
    "\n",
    "            # Detect left-click (if index finger touches thumb)\n",
    "            if abs(index_x - thumb_x) < 20 and abs(index_y - thumb_y) < 20:\n",
    "                #print(\"Left Click\")\n",
    "                mouse.click(Button.left, 1)\n",
    "\n",
    "            # Detect right-click (if index finger touches middle finger)\n",
    "            if abs(index_x - middle_x) < 20 and abs(index_y - middle_y) < 20:\n",
    "                #print(\"Right Click\")\n",
    "                mouse.click(Button.right, 1)\n",
    "\n",
    "            # Update previous coordinates for swipe detection\n",
    "            prev_x, prev_y = index_x, index_y\n",
    "\n",
    "            # Draw hand landmarks on the frame\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "    # Show the image\n",
    "    cv2.imshow('Hand Tracking', frame)\n",
    "\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Clean up\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6ae52e3-3899-4837-941c-6588ee2df434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "from pynput.mouse import Button, Controller as MouseController\n",
    "from pynput.keyboard import Controller as KeyboardController, Key\n",
    "import time\n",
    "\n",
    "# Initialize mediapipe hands and drawing utils\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mouse = MouseController()\n",
    "keyboard = KeyboardController()\n",
    "\n",
    "# Setup hand tracking\n",
    "hands = mp_hands.Hands(\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.7,\n",
    "    min_tracking_confidence=0.7\n",
    ")\n",
    "\n",
    "# Function to check if fingers are close enough for a specific action\n",
    "def fingers_close_to_landmarks(landmarks, landmark_indices):\n",
    "    distances = []\n",
    "    for index in landmark_indices:\n",
    "        finger_tip = landmarks.landmark[index]\n",
    "        distances.append(finger_tip.y)  # Consider y-coordinates for closeness\n",
    "\n",
    "    # Check if the y-coordinates are within a certain threshold\n",
    "    return max(distances) - min(distances) < 0.05  # Adjust threshold as needed\n",
    "\n",
    "# Function to detect swipe direction based on two fingers\n",
    "def detect_swipe_direction(index_pos, middle_pos):\n",
    "    if index_pos.x < middle_pos.x:\n",
    "        return \"left\"\n",
    "    elif index_pos.x > middle_pos.x:\n",
    "        return \"right\"\n",
    "    return None\n",
    "\n",
    "# Open webcam feed\n",
    "cap = cv2.VideoCapture(0)\n",
    "prev_swipe_time = time.time()  # To control cooldown for swipes\n",
    "cooldown_duration = 1.0  # 1 second cooldown\n",
    "space_pressed = False  # Track if space has been pressed\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        print(\"Ignoring empty frame.\")\n",
    "        continue\n",
    "\n",
    "    # Flip the image horizontally for a selfie-view display\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    h, w, _ = frame.shape\n",
    "\n",
    "    # Convert the frame to RGB\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the frame and get hand landmarks\n",
    "    result = hands.process(rgb_frame)\n",
    "\n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand_landmarks in result.multi_hand_landmarks:\n",
    "            # Get landmark coordinates\n",
    "            index_finger = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "            middle_finger = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_TIP]\n",
    "            thumb = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP]\n",
    "            landmarks_list = [mp_hands.HandLandmark.PINKY_TIP, \n",
    "                              mp_hands.HandLandmark.RING_FINGER_TIP,\n",
    "                              mp_hands.HandLandmark.MIDDLE_FINGER_TIP, \n",
    "                              mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "\n",
    "            # Get the coordinates in pixels\n",
    "            index_x, index_y = int(index_finger.x * w), int(index_finger.y * h)\n",
    "            middle_x, middle_y = int(middle_finger.x * w), int(middle_finger.y * h)\n",
    "            thumb_x, thumb_y = int(thumb.x * w), int(thumb.y * h)\n",
    "\n",
    "            # Move the cursor based on index finger position\n",
    "            mouse.position = (index_x, index_y)\n",
    "\n",
    "            # Detect swipe direction\n",
    "            swipe_direction = detect_swipe_direction(index_finger, middle_finger)\n",
    "\n",
    "            # Swipe detection\n",
    "            if swipe_direction:\n",
    "                current_time = time.time()\n",
    "                if current_time - prev_swipe_time > cooldown_duration:  # Check cooldown\n",
    "                    if swipe_direction == \"right\":\n",
    "                        keyboard.press(Key.right)\n",
    "                        keyboard.release(Key.right)\n",
    "                    elif swipe_direction == \"left\":\n",
    "                        keyboard.press(Key.left)\n",
    "                        keyboard.release(Key.left)\n",
    "                    prev_swipe_time = current_time  # Reset swipe time\n",
    "\n",
    "            # Check for scrolling\n",
    "            distance_thumb = abs(index_y - thumb_y)\n",
    "            if distance_thumb < 40:  # Thumb is close to index finger\n",
    "                mouse.scroll(0, 2)  # Scroll up\n",
    "            elif distance_thumb > 90:  # Thumb is far from index finger\n",
    "                mouse.scroll(0, -2)  # Scroll down\n",
    "\n",
    "            # Detect left-click (if index finger touches thumb)\n",
    "            if abs(index_x - thumb_x) < 20 and abs(index_y - thumb_y) < 20:\n",
    "                mouse.click(Button.left, 1)\n",
    "\n",
    "            # Detect right-click (if index finger touches middle finger)\n",
    "            if abs(index_x - middle_x) < 20 and abs(index_y - middle_y) < 20:\n",
    "                mouse.click(Button.right, 1)\n",
    "\n",
    "            # Check for spacebar press condition\n",
    "            if fingers_close_to_landmarks(hand_landmarks, landmarks_list) and not space_pressed:\n",
    "                keyboard.press(Key.space)\n",
    "                keyboard.release(Key.space)\n",
    "                space_pressed = True  # Prevent multiple space presses\n",
    "            elif not fingers_close_to_landmarks(hand_landmarks, landmarks_list):\n",
    "                space_pressed = False  # Reset if fingers move away\n",
    "\n",
    "            # Draw hand landmarks on the frame\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "    # Show the image\n",
    "    cv2.imshow('Hand Tracking', frame)\n",
    "\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Clean up\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adb0bb24-86a9-4fb4-ace0-fd11e0f0673b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "from pynput.mouse import Controller as MouseController\n",
    "from pynput.keyboard import Controller as KeyboardController, Key\n",
    "import time\n",
    "\n",
    "# Initialize mediapipe hands and drawing utils\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mouse = MouseController()\n",
    "keyboard = KeyboardController()\n",
    "\n",
    "# Setup hand tracking\n",
    "hands = mp_hands.Hands(\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.7,\n",
    "    min_tracking_confidence=0.7\n",
    ")\n",
    "\n",
    "# Gesture tracking variables\n",
    "gesture_performed = False\n",
    "last_gesture = None  # Store last gesture to avoid repetition\n",
    "last_gesture_time = 0  # Time of the last gesture\n",
    "cooldown_duration = 1.0  # Cooldown time (in seconds)\n",
    "action_text = \"\"  # To display the current action in the window\n",
    "\n",
    "# Helper function to detect if all fingers (except thumb) are up or down\n",
    "def all_fingers_up_down(landmarks, direction='up'):\n",
    "    finger_tips = [\n",
    "        mp_hands.HandLandmark.INDEX_FINGER_TIP,\n",
    "        mp_hands.HandLandmark.MIDDLE_FINGER_TIP,\n",
    "        mp_hands.HandLandmark.RING_FINGER_TIP,\n",
    "        mp_hands.HandLandmark.PINKY_TIP\n",
    "    ]\n",
    "    \n",
    "    for tip in finger_tips:\n",
    "        if direction == 'up' and landmarks.landmark[tip].y > landmarks.landmark[mp_hands.HandLandmark.WRIST].y:\n",
    "            return False  # Fingers are not up\n",
    "        elif direction == 'down' and landmarks.landmark[tip].y < landmarks.landmark[mp_hands.HandLandmark.WRIST].y:\n",
    "            return False  # Fingers are not down\n",
    "    return True\n",
    "\n",
    "# Helper function to detect if all fingers (except thumb) swipe left or right\n",
    "def all_fingers_swiped(landmarks, direction='right'):\n",
    "    finger_tips = [\n",
    "        mp_hands.HandLandmark.INDEX_FINGER_TIP,\n",
    "        mp_hands.HandLandmark.MIDDLE_FINGER_TIP,\n",
    "        mp_hands.HandLandmark.RING_FINGER_TIP,\n",
    "        mp_hands.HandLandmark.PINKY_TIP\n",
    "    ]\n",
    "    \n",
    "    for tip in finger_tips:\n",
    "        if direction == 'right' and landmarks.landmark[tip].x < landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP].x:\n",
    "            return False  # Fingers are not swiping right\n",
    "        elif direction == 'left' and landmarks.landmark[tip].x > landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP].x:\n",
    "            return False  # Fingers are not swiping left\n",
    "    return True\n",
    "\n",
    "# Helper function to reset the gesture state to neutral\n",
    "def hand_in_neutral(landmarks):\n",
    "    index_tip = landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "    wrist = landmarks.landmark[mp_hands.HandLandmark.WRIST]\n",
    "    \n",
    "    # Hand is neutral when the index finger is close to the wrist (neutral position)\n",
    "    return (index_tip.x == wrist.x) and (index_tip.y == wrist.y)\n",
    "\n",
    "# Open webcam feed\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        print(\"Ignoring empty frame.\")\n",
    "        continue\n",
    "\n",
    "    # Flip the image horizontally for a selfie-view display\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    h, w, _ = frame.shape\n",
    "\n",
    "    # Convert the frame to RGB\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the frame and get hand landmarks\n",
    "    result = hands.process(rgb_frame)\n",
    "\n",
    "    # Check if the hand is in front of the camera\n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand_landmarks in result.multi_hand_landmarks:\n",
    "            # Get landmark coordinates\n",
    "            index_finger = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "            palm_base = hand_landmarks.landmark[mp_hands.HandLandmark.WRIST]\n",
    "\n",
    "            # Cursor control using index finger tip\n",
    "            index_x, index_y = int(index_finger.x * w), int(index_finger.y * h)\n",
    "            mouse.position = (index_x, index_y)\n",
    "\n",
    "            if time.time() - last_gesture_time > cooldown_duration:\n",
    "                # Scroll Up: All fingers (except thumb) swiped down\n",
    "                if all_fingers_up_down(hand_landmarks, 'down') and last_gesture != 'scroll_up':\n",
    "                    action_text = \"Scroll Up\"\n",
    "                    mouse.scroll(0, 2)\n",
    "                    gesture_performed = True\n",
    "                    last_gesture = 'scroll_up'\n",
    "                    last_gesture_time = time.time()\n",
    "\n",
    "                # Scroll Down: All fingers (except thumb) swiped up\n",
    "                elif all_fingers_up_down(hand_landmarks, 'up') and last_gesture != 'scroll_down':\n",
    "                    action_text = \"Scroll Down\"\n",
    "                    mouse.scroll(0, -2)\n",
    "                    gesture_performed = True\n",
    "                    last_gesture = 'scroll_down'\n",
    "                    last_gesture_time = time.time()\n",
    "\n",
    "                # Swipe Right: All fingers (except thumb) swiped right\n",
    "                elif all_fingers_swiped(hand_landmarks, 'right') and last_gesture != 'swipe_right':\n",
    "                    action_text = \"Swipe Right\"\n",
    "                    keyboard.press(Key.right)\n",
    "                    keyboard.release(Key.right)\n",
    "                    gesture_performed = True\n",
    "                    last_gesture = 'swipe_right'\n",
    "                    last_gesture_time = time.time()\n",
    "\n",
    "                # Swipe Left: All fingers (except thumb) swiped left\n",
    "                elif all_fingers_swiped(hand_landmarks, 'left') and last_gesture != 'swipe_left':\n",
    "                    action_text = \"Swipe Left\"\n",
    "                    keyboard.press(Key.left)\n",
    "                    keyboard.release(Key.left)\n",
    "                    gesture_performed = True\n",
    "                    last_gesture = 'swipe_left'\n",
    "                    last_gesture_time = time.time()\n",
    "\n",
    "                # Press Space: All fingers close to the palm base\n",
    "                distance_to_palm = [\n",
    "                    abs(hand_landmarks.landmark[finger].y - palm_base.y)\n",
    "                    for finger in [\n",
    "                        mp_hands.HandLandmark.INDEX_FINGER_TIP,\n",
    "                        mp_hands.HandLandmark.MIDDLE_FINGER_TIP,\n",
    "                        mp_hands.HandLandmark.RING_FINGER_TIP,\n",
    "                        mp_hands.HandLandmark.PINKY_TIP\n",
    "                    ]\n",
    "                ]\n",
    "                if all(d < 0.05 for d in distance_to_palm) and last_gesture != 'space_press':\n",
    "                    action_text = \"Space Bar Pressed\"\n",
    "                    keyboard.press(Key.space)\n",
    "                    keyboard.release(Key.space)\n",
    "                    gesture_performed = True\n",
    "                    last_gesture = 'space_press'\n",
    "                    last_gesture_time = time.time()\n",
    "\n",
    "            # Draw hand landmarks on the frame\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "    else:\n",
    "        # Reset gesture when hand is removed from view\n",
    "        action_text = \"Hand Not Detected - Reset\"\n",
    "        gesture_performed = False\n",
    "        last_gesture = None\n",
    "\n",
    "    # Display the action text on the frame\n",
    "    cv2.putText(frame, action_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    # Show the image\n",
    "    cv2.imshow('Hand Tracking', frame)\n",
    "\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Clean up\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf170407-c27d-4edb-b0ac-99c998db18bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\souja\\anaconda3\\envs\\project\\lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "import time\n",
    "\n",
    "# Initialize mediapipe hands and drawing utils\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Setup hand tracking\n",
    "hands = mp_hands.Hands(\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.7,\n",
    "    min_tracking_confidence=0.7\n",
    ")\n",
    "\n",
    "# Function to check if fingers are close enough for a specific action\n",
    "def fingers_close_to_landmarks(landmarks, landmark_indices):\n",
    "    distances = []\n",
    "    for index in landmark_indices:\n",
    "        finger_tip = landmarks.landmark[index]\n",
    "        distances.append(finger_tip.y)  # Consider y-coordinates for closeness\n",
    "\n",
    "    # Check if the y-coordinates are within a certain threshold\n",
    "    return max(distances) - min(distances) < 0.05  # Adjust threshold as needed\n",
    "\n",
    "# Function to detect swipe direction based on two fingers\n",
    "def detect_swipe_direction(index_pos, middle_pos):\n",
    "    if index_pos.x < middle_pos.x:\n",
    "        return \"left\"\n",
    "    elif index_pos.x > middle_pos.x:\n",
    "        return \"right\"\n",
    "    return None\n",
    "\n",
    "# Open webcam feed\n",
    "cap = cv2.VideoCapture(0)\n",
    "prev_swipe_time = time.time()  # To control cooldown for swipes\n",
    "cooldown_duration = 1.0  # 1 second cooldown\n",
    "space_pressed = False  # Track if space has been pressed\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        print(\"Ignoring empty frame.\")\n",
    "        continue\n",
    "\n",
    "    # Flip the image horizontally for a selfie-view display\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    h, w, _ = frame.shape\n",
    "\n",
    "    # Convert the frame to RGB\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the frame and get hand landmarks\n",
    "    result = hands.process(rgb_frame)\n",
    "\n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand_landmarks in result.multi_hand_landmarks:\n",
    "            # Get landmark coordinates\n",
    "            index_finger = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "            middle_finger = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_TIP]\n",
    "            thumb = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP]\n",
    "            landmarks_list = [mp_hands.HandLandmark.PINKY_TIP, \n",
    "                              mp_hands.HandLandmark.RING_FINGER_TIP,\n",
    "                              mp_hands.HandLandmark.MIDDLE_FINGER_TIP, \n",
    "                              mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "\n",
    "            # Get the coordinates in pixels\n",
    "            index_x, index_y = int(index_finger.x * w), int(index_finger.y * h)\n",
    "            middle_x, middle_y = int(middle_finger.x * w), int(middle_finger.y * h)\n",
    "            thumb_x, thumb_y = int(thumb.x * w), int(thumb.y * h)\n",
    "\n",
    "            # Move the cursor based on index finger position\n",
    "            pyautogui.moveTo(index_x, index_y)\n",
    "\n",
    "            # Detect swipe direction\n",
    "            swipe_direction = detect_swipe_direction(index_finger, middle_finger)\n",
    "\n",
    "            # Swipe detection\n",
    "            if swipe_direction:\n",
    "                current_time = time.time()\n",
    "                if current_time - prev_swipe_time > cooldown_duration:  # Check cooldown\n",
    "                    if swipe_direction == \"right\":\n",
    "                        pyautogui.hotkey('right')\n",
    "                    elif swipe_direction == \"left\":\n",
    "                        pyautogui.hotkey('left')\n",
    "                    prev_swipe_time = current_time  # Reset swipe time\n",
    "\n",
    "            # Check for scrolling\n",
    "            distance_thumb = abs(index_y - thumb_y)\n",
    "            if distance_thumb < 40:  # Thumb is close to index finger\n",
    "                pyautogui.scroll(2)  # Scroll up\n",
    "            elif distance_thumb > 90:  # Thumb is far from index finger\n",
    "                pyautogui.scroll(-2)  # Scroll down\n",
    "\n",
    "            # Detect left-click (if index finger touches thumb)\n",
    "            if abs(index_x - thumb_x) < 20 and abs(index_y - thumb_y) < 20:\n",
    "                pyautogui.click(button='left')\n",
    "\n",
    "            # Detect right-click (if index finger touches middle finger)\n",
    "            if abs(index_x - middle_x) < 20 and abs(index_y - middle_y) < 20:\n",
    "                pyautogui.click(button='right')\n",
    "\n",
    "            # Check for spacebar press condition\n",
    "            if fingers_close_to_landmarks(hand_landmarks, landmarks_list) and not space_pressed:\n",
    "                pyautogui.press('space')\n",
    "                space_pressed = True  # Prevent multiple space presses\n",
    "            elif not fingers_close_to_landmarks(hand_landmarks, landmarks_list):\n",
    "                space_pressed = False  # Reset if fingers move away\n",
    "\n",
    "            # Draw hand landmarks on the frame\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "    # Show the image\n",
    "    cv2.imshow('Hand Tracking', frame)\n",
    "\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Clean up\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaee9f0-44d6-43f2-b5bf-6b50e5cd0181",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
